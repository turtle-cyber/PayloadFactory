INFO:__main__:Found 3 exploits to optimize.
INFO:ml_engine.db_manager:Connected to MongoDB: payloadfactoryDB
INFO:ml_engine.fuzzing_module:Fuzzer initialized for target: 192.168.1.157:8080
2025-12-03 17:41:16,593 - INFO - Optimizing exploit_BackportEnglish.java_0_20251203_172229.py...
INFO:__main__:Optimizing exploit_BackportEnglish.java_0_20251203_172229.py...
2025-12-03 17:41:16,593 - INFO -   -> Running Fuzzer (Attempt 1/3)...
INFO:__main__:  -> Running Fuzzer (Attempt 1/3)...
INFO:ml_engine.fuzzing_module:Starting fuzzing session with 20 iterations.
WARNING:ml_engine.fuzzing_module:Iteration 5: High Latency (2012.92ms) detected!
WARNING:ml_engine.fuzzing_module:Iteration 16: High Latency (2028.54ms) detected!
2025-12-03 17:41:22,888 - WARNING -   -> Fuzzing failed (0 crashes). Initiating Self-Healing...
WARNING:__main__:  -> Fuzzing failed (0 crashes). Initiating Self-Healing...
INFO:ml_engine.exploit_generator:Loading Hermes 3 model from E:\GRAND_AI_MODELS\hermes-3-llama-3.1-8b on cuda (4-bit quantized)
2025-12-03 17:41:28.536714: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2025-12-03 17:41:31.678497: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.75s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.85s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.91s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.30s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:15<00:00,  3.87s/it]
INFO:ml_engine.exploit_generator:Regenerating exploit due to failure: No Crash detected. Payload might be incorrect or offset too small.
Setting `pad_token_id` to `eos_token_id`:128040 for open-end generation.
INFO:ml_engine.exploit_generator:Extracted payload: 64 bytes
2025-12-03 17:43:17,122 - INFO -   -> Exploit regenerated and saved. Retrying...
INFO:__main__:  -> Exploit regenerated and saved. Retrying...
2025-12-03 17:43:17,122 - INFO -   -> Running Fuzzer (Attempt 2/3)...
INFO:__main__:  -> Running Fuzzer (Attempt 2/3)...
INFO:ml_engine.fuzzing_module:Starting fuzzing session with 20 iterations.
WARNING:ml_engine.fuzzing_module:Iteration 2: High Latency (2009.72ms) detected!
2025-12-03 17:43:21,318 - WARNING -   -> Fuzzing failed (0 crashes). Initiating Self-Healing...
WARNING:__main__:  -> Fuzzing failed (0 crashes). Initiating Self-Healing...
INFO:ml_engine.exploit_generator:Loading Hermes 3 model from E:\GRAND_AI_MODELS\hermes-3-llama-3.1-8b on cuda (4-bit quantized)
INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).

Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
Loading checkpoint shards:  25%|██▌       | 1/4 [00:04<00:14,  4.72s/it]
Loading checkpoint shards:  50%|█████     | 2/4 [00:09<00:09,  4.97s/it]
Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:04,  4.93s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  3.66s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [00:16<00:00,  4.12s/it]
INFO:ml_engine.exploit_generator:Regenerating exploit due to failure: No Crash detected. Payload might be incorrect or offset too small.
Setting `pad_token_id` to `eos_token_id`:128040 for open-end generation.
