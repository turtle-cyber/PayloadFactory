import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ExploitGenerator:
    def __init__(self, model_path=r"E:\GRAND_AI_MODELS\hermes-3-llama-3.1-8b"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Loading Hermes 3 model from {model_path} on {self.device} (4-bit quantized)")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path)
            
            # 4-bit Quantization Config
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_use_double_quant=True,
            )

            self.model = AutoModelForCausalLM.from_pretrained(
                model_path, 
                quantization_config=quantization_config,
                device_map="auto"
            )
        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            raise e

    def generate_exploit(self, vulnerability_details):
        """
        Generates an exploit script based on the provided vulnerability details.
        Uses an enhanced prompt with multi-stage reasoning.
        """
        logger.info("Generating exploit...")
        
        # Enhanced Alpaca Prompt with Detailed Reasoning
        prompt = f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
You are an expert exploit developer and binary security researcher. Analyze the following vulnerability and generate a complete, production-ready Python exploit using the pwntools framework.

**Vulnerability Details:**
{vulnerability_details}

**Your Task:**
Generate a Python exploit script that follows this multi-stage reasoning process:

**Stage 1: Analysis**
- Determine the vulnerability type (buffer overflow, format string, use-after-free, etc.)
- Identify the target architecture (32-bit or 64-bit) from the address patterns
  * 32-bit addresses: typically 0x08xxxxxx or 0xffffxxxx (8 hex digits)
  * 64-bit addresses: typically 0x00007fxxxxxxxxxx or 0x0000xxxxxxxxxxxx (12-16 hex digits)
- Detect the service type (HTTP, FTP, raw TCP, etc.) from context clues
- Note any crash addresses (e.g., 0x41414141 indicates buffer overflow with 'A' characters)

**Stage 2: Exploit Construction**
1. **Use correct packing functions:**
   - Use `p32()` for 32-bit targets
   - Use `p64()` for 64-bit targets
   
2. **Calculate offsets:**
   - If the vulnerability mentions an offset, use it
   - If a crash address like 0x41414141 is present, calculate the cyclic pattern offset
   - Include a comment explaining the offset calculation
   
3. **Build the payload strategically:**
   - Padding to reach the return address
   - ROP gadgets (if applicable) or shellcode address
   - For stack-based exploits, include a NOP sled if using shellcode
   
4. **Protocol-aware delivery:**
   - For HTTP: Craft proper GET/POST requests with malicious headers/data
   - For raw TCP: Use direct socket send
   - For FTP: Use FTP commands (USER, PASS, etc.)
   
5. **Include exploitation techniques:**
   - Mention DEP/NX bypass strategies (ROP chains)
   - ASLR considerations (info leaks, partial overwrites)
   - Stack canaries (if detected, note bypass strategies)

**Stage 3: Verification**
- Add proper error handling
- Include interactive shell access via `target.interactive()`
- Add logging/debugging output

**Output Format:**
Provide ONLY the Python code wrapped in ```python``` tags. Include detailed comments explaining each section.

### Response:
```python
"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs, 
                max_new_tokens=768,  # Increased for more detailed output
                temperature=0.5,      # Lower temperature for more focused output
                do_sample=True,
                top_p=0.9
            )
            
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extract code block - look for python code markers
        if "```python" in generated_text:
            exploit_code = generated_text.split("```python")[-1].split("```")[0].strip()
        elif "### Response:" in generated_text:
            # Fallback: extract everything after Response marker
            exploit_code = generated_text.split("### Response:")[-1].strip()
        else:
            exploit_code = generated_text
             
        return exploit_code

if __name__ == "__main__":
    # Note: This requires the actual model path to be valid and accessible.
    try:
        generator = ExploitGenerator()
        vuln = "Buffer overflow in 'login' function at offset 64."
        print(generator.generate_exploit(vuln))
    except Exception as e:
        print(f"Skipping test due to model load failure: {e}")
